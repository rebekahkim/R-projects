---
title: "STAT UN2103 HW5"
author: "Rebekah Kim"
date: "November 29, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(leaps)
library(bestglm)
library(car)
require(gridExtra)
```

## I. Introduction
### a. Goal of analysis
#### Research Questions
> 1. Do Blacks have statistically different wages compared to Whites?
> 2. Do Blacks have statistically different wages compared to all other races?

### b. Setup
```{r}
df <- read.csv("salary.txt", header = T)

set.seed(0)
index <- sample(1:nrow(df),4965,replace = F)
train.data <- df[-index,]
data <- train.data 
test.data <- df[index,]
```

### c. Exploratory Data Analysis with Interaction Plots
```{r message=FALSE}
head(df)
str(df)


# Race (race) by Region (reg)
ggplot(data, aes(x = race, y = log(wage)))+
  geom_boxplot() +
  facet_wrap(~reg, nrow = 1)

# Experience (exp)- interaction with Race
plot1 <- ggplot(data, aes(x = exp, y = log(wage), group = race)) +
  geom_smooth(aes(linetype = race, color = race))

# Commute distance (com)
plot2 <- ggplot(data, aes(x = com, y = log(wage))) +
  geom_point() +
  geom_smooth(aes(linetype = race, color = race))

# Education years (edu)
plot3<- ggplot(data, aes(x = edu, y = log(wage))) +
  geom_point() +
  geom_smooth(aes(linetype = race, color = race)) 

# Number of Employees (emp)
plot4 <- ggplot(data, aes(x = emp, y = log(wage))) +
  geom_point()+
  geom_smooth(aes(group = race, linetype = race, color = race))

grid.arrange(plot1, plot2, plot3, plot4, ncol=2)

# City (city) by Race (race)
plot5 <- ggplot(data, aes(x = city, y = log(wage))) +
  geom_boxplot() +
  facet_wrap(~race, nrow = 1)

# Degree (deg) by Race (race)
plot6<- ggplot(data, aes(x = deg, y = log(wage))) +
  geom_boxplot() +
  facet_wrap(~race, nrow = 1)

grid.arrange(plot5, plot6, nrow=1)


par(mfrow=c(2,4))

exp <- data$exp
edu <- data$edu
city <- data$city
race <- data$race
reg <- data$reg
emp <- data$emp
wage <- data$wage

# City vs. region (yes)
interaction.plot(city,reg,log(wage))


# race vs. region- lines cross
interaction.plot(race,reg,log(wage))


# race vs. city- slightly different slopes (included)
interaction.plot(race,city,log(wage))

#exp:edu

#interaction.plot(exp, edu, log(wage))

#city:edu 
interaction.plot(city, edu, log(wage))

#exp:reg 
interaction.plot(reg, exp, log(wage))

#reg:edu 
interaction.plot(reg, edu, log(wage))

#exp:city 
interaction.plot(city, exp, log(wage))

# race:emp
interaction.plot(race, emp, log(wage))

```

## II. Statistical Model
```{r}
# Final model of choice 
# log transformation of response, quadratic edu and exp, and interactions added
model <- lm(formula = log(wage) ~ I(edu^2) + exp + I(exp^2) + city + race + 
    reg + emp + edu + exp:edu + city:edu + exp:reg + city:reg + 
    reg:edu + exp:city + race:emp + city:race, data = data)
summary(model)
summary(model)$r.squared
summary(model)$adj.r.squared
extractAIC(model)
```

## III. Research Question
### a. Setup
$H_0 : \beta_{white}=0$

$H_0 : \beta_{other}=0$ 

$H_0 : \beta_{other}= \beta_{white}$

### b. Discussion & Conclusion

It would have been interesting to have data on what types of jobs blacks and other race groups had and compare the wages within the same job. Obviously, doctors get paid more than janitors; it would be interesting to compare the wages of black doctors and white doctors. Then, we would be able to study the wage gap that may exist between racial groups doing similar job. Another interesting follow-up study would be to observe if there is job discrimination, maybe analyzing how the rates of different race groups entering high-paying professions compare.

With this study, we cannot know the reason behind pay discrepancy. But it seems like whether blacks have statistically different wages compared to whites and all race groups depends on other variables (interactions). The summary output shows that there is no significant interaction between race and number of employees in company. However, there is significant interacton between race and city-living - 

## IV. Appendix

### a. Model Selection
#### Transformation
The QQ-plot below that shows the untransformed response looks very bad (barely aligns with the 45 degree line). The model's QQ-plot shows that a log transformation of the response is a good choice. 

```{r}
par(mfrow=c(2,3))
qqnorm(rstudent(lm(wage~edu+exp+city+reg+race+deg+com+emp, data=data)))
qqnorm(rstudent(model))
```

#### Functional Forms
According to EDA, experience and education seem to have quadratic forms. Thus, both predictor variables had second-order terms included in the final model.

#### Unselected/Selected Variables Based on EDA and Intuition
The basic model where all predictors were used didn't do very well.
The commute distance did not seem to have an effect the response, according to EDA. 
The college degree information is included in the edu data.

Education level is a well-supported predictor for wage. The number of employees serves as a proxy for company size. Usually, large corporations pay more, so it should be included in the model. The EDA plot of log(wage) vs. emp indeed shows a slight upward linear trend for all racial groups. 

#### Interactions
I originally looked at various interaction plots to decide which interaction terms to include. Then I got curious and wrote a small Python script to generate all subsets of the 8 predictor variables, then ran a stepwise regression on it to come up with the final model. I got rid of interactions that were more than two-way, since they only boosted the adjusted R-squared by a little bit and are not very interpretable, and got the current model.

#### Other Model Candidates
```{r}
# Candidate 1: basic model
summary(lm(wage~edu+exp+city+reg+race+deg+emp+com,data=train.data))$adj.r.squared
extractAIC(lm(wage~edu+exp+city+reg+race+deg+emp+com,data=train.data))

# Candidate 2: log transformation of response
summary(lm(log(wage)~edu+exp+city+reg+race+deg+emp+com,data=train.data))$adj.r.squared
extractAIC(lm(log(wage)~edu+exp+city+reg+race+deg+emp+com,data=train.data))

# Candidate 3: polynomials for edu and exp added
summary(lm(log(wage)~edu+I(edu^2)+exp+I(exp^2)+city+reg+race+deg,data=train.data))$adj.r.squared
extractAIC(lm(log(wage)~edu+I(edu^2)+exp+I(exp^2)+city+reg+race+deg,data=train.data))
```

In the end, my final model has the highest unadjusted (0.3546687) and adjusted R-squared (0.35379) and the lowest AIC (-26671.65) among the above three.


### b. Diagnostics and Model Validation

```{r}
city <- I(data$city=="yes")
white <- I(data$race=="white")
other <- I(data$race=="other")
northeast <- I(data$reg=="northeast")
south <- I(data$reg=="south")
west <- I(data$reg=="west")

lwage <- I(log(data$wage))
lineaeexp <- poly(data$exp,2)[,1] 
exp <- I(data$exp) # same as above
quadexp <- poly(data$exp,2)[,2]
lineaeedu <- poly(data$edu,2)[,1] 
edu <- I(data$edu) #same as above
quadedu <- poly(data$edu,2)[,2]

emp <- I(data$emp)
exp.edu <- I(exp*edu) 
cityyes.edu <- I(city*edu) 
exp.south <- I(exp*south)
exp.northeast <- I(exp*northeast)
exp.west <- I(exp*west)

cityyes.south <- I(city*south)
cityyes.west <- I(city*west)
cityyes.northeast <- I(city*northeast)

south.edu <- I(south*edu)
northeast.edu <- I(northeast*edu)
west.edu <- I(west*edu)

exp.cityyes <- I(exp*city)
white.emp <- I(white*emp)
other.emp <- I(other*emp)
white.cityyes <- I(city*white)
other.cityyes <- I(city*other)

#same as the original final model- just manually inputted predictor
model.manual <- lm(lwage~lineaeexp +quadexp +lineaeedu +quadedu  +city +other +white +northeast +south +west +emp +exp.edu +cityyes.edu +exp.south +exp.northeast +exp.west +cityyes.south +cityyes.west +cityyes.northeast +south.edu +northeast.edu +west.edu +exp.cityyes +white.emp +other.emp +white.cityyes +other.cityyes)

# Define test data frame for test data
city.test <- I(test.data$city=="yes")
white.test <- I(test.data$race=="white")
other.test <- I(test.data$race=="other")
northeast.test <- I(test.data$reg=="northeast")
south.test <- I(test.data$reg=="south")
west.test <- I(test.data$reg=="west")

lwage.test <- I(log(test.data$wage))
lineaeexp.test <- poly(test.data$exp,2)[,1] 
quadexp.test <- poly(test.data$exp,2)[,2]
lineaeedu.test <- poly(test.data$edu,2)[,1] 
quadedu.test <- poly(test.data$edu,2)[,2]
emp.test <- I(test.data$emp)

exp.edu.test <- I(lineaeexp.test*lineaeedu.test) 
cityyes.edu.test <- I(city.test*lineaeedu.test) 
exp.south.test <- I(lineaeexp.test*south.test)
exp.northeast.test <- I(lineaeexp.test*northeast.test)
exp.west.test <- I(lineaeexp.test*west.test)

cityyes.south.test <- I(city.test*south.test)
cityyes.west.test <- I(city.test*west.test)
cityyes.northeast.test <- I(city.test*northeast.test)

south.edu.test <- I(south.test*lineaeedu.test)
northeast.edu.test <- I(northeast.test*lineaeedu.test)
west.edu.test <- I(west.test*lineaeedu.test)

exp.cityyes.test <- I(lineaeexp.test*city.test)
white.emp.test <- I(white.test*emp.test)
other.emp.test <- I(other.test*emp.test)
white.cityyes.test <- I(city.test*white.test)
other.cityyes.test <- I(city.test*other.test)


X.test <- data.frame(city = city.test, 
white = white.test, 
other= other.test, 
northeast= northeast.test, 
south = south.test, 
west= west.test, 
lineaeexp= lineaeexp.test, 
quadexp= quadexp.test, 
lineaeedu= lineaeedu.test, 
quadedu= quadedu.test, 
emp= emp.test, 
exp.edu = exp.edu.test, 
cityyes.edu= cityyes.edu.test, 
exp.south= exp.south.test, 
exp.northeast= exp.northeast.test, 
exp.west= exp.west.test, 
cityyes.south= cityyes.south.test, 
cityyes.west= cityyes.west.test, 
cityyes.northeast= cityyes.northeast.test, 
south.edu= south.edu.test, 
northeast.edu = northeast.edu.test, 
west.edu = west.edu.test, 
exp.cityyes= exp.cityyes.test, 
white.emp= white.emp.test, 
other.emp= other.emp.test, 
white.cityyes= white.cityyes.test, 
other.cityyes= other.cityyes.test)


y.hat.mspr <- predict(model.manual,newdata = X.test)

# Compute MSPR
n.star <- nrow(test.data)

MSPR <- 1/n.star*sum((lwage.test-y.hat.mspr)^2)

MSE <- (summary(lm(model.manual))$sigma)^2

# Compare MSPR  to MSE
MSPR
MSE

MSPR/MSE
```

The MSPR and MSE are reasonably close; they are in the same order of magnitude. 

### c. Influential Observations and Collinearity
DFBETAS would be more appropriate here since we are focusing on answering the research questions (testing slopes), not predicting. 

```{r}
n <- nrow(data)
x <- seq(1, nrow(data))
y.intc <- 2/sqrt(n)
         
dfplot1 <-ggplot(data, aes(x =x, y = dfbetas(model)[,6])) +
  geom_point() +
  geom_hline(aes(yintercept = y.intc, col= "red")) +
  geom_hline(aes(yintercept = -y.intc, col = "red"))+ theme(legend.position="none") + labs(title="DFBETAS-Race:Other")
      
dfplot2 <-ggplot(data, aes(x =x, y = dfbetas(model)[,7])) +
  geom_point() +
  geom_hline(aes(yintercept = y.intc, col= "red")) +
  geom_hline(aes(yintercept = -y.intc, col = "red"))+ theme(legend.position="none")+labs(title="DFBETAS-Race:White")

grid.arrange(dfplot1,dfplot2, nrow=1)
```

As expected from the size of the given data set, there are many influential observations. It may be wise to come up with a different model. 

```{r}
mean(vif(model)[,3])

numeric_preds <- data.frame(lwage=lwage, edu=edu, exp=exp, edu.sq = quadedu, exp.sq = quadexp, exp.edu = exp.edu,  emp=emp)
cor(numeric_preds)
             
```
The average VIF 4.605 > 1. This is probably because I have many interaction terms. For example, the interaction exp:edu will probably be correlated to exp and edu. Upon inspecting the correlation between the numeric predictors, the exp.edu to edu and exp have high correlation, at around 0.87. However, otherwise, the correlation matrix does not look bad. It may be good to try PCA or ridge regression. 

## d. Other Ideas on Model Selection
```{r}
summary(lm(formula = log(wage) ~ I(edu^2) + exp + I(exp^2) + city + race +
  emp + reg + deg + edu + city:reg + exp:city + exp:reg + exp:emp +
  city:deg + exp:deg + race:emp + race:deg + reg:deg + city:edu +
  exp:edu + reg:edu + exp:race + race:edu + emp:edu + exp:city:edu +
  city:reg:edu + race:emp:edu + exp:race:edu, data = data))$adj.r.squared
```
This model had the highest adjusted R-squared, but I didn't employ this as the final model because it is far from parsimonious or interpretable. Besides, it is only an 0.02% increase in adjusted R-squred from my final model.

A way to increase adjusted R-squared would be to brute force it, where you try every possible combination of interactions and higher order terms and keep your computer running forever until you reach a target value (say, 50%). It would be interesting to see what model you would get, but I assume it won't be very interpretable.
