---
title: "STAT UN2103 HW5"
author: "Rebekah Kim"
date: "November 29, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(leaps)
library(bestglm)
library(car)
require(gridExtra)
```

## I. Introduction
### a. Goal of analysis
#### Research Questions
> 1. Do Blacks have statistically different wages compared to Whites?
> 2. Do Blacks have statistically different wages compared to all other races?

### b. Setup
```{r}
df <- read.csv("salary.txt", header = T)

set.seed(0)
index <- sample(1:nrow(df),4965,replace = F)
train.data <- df[-index,]
data <- train.data 
test.data <- df[index,]
```

### c. Exploratory Data Analysis with Interaction Plots
```{r message=FALSE}
head(df)
str(df)


# Race (race) by Region (reg)
ggplot(data, aes(x = race, y = log(wage)))+
  geom_boxplot() +
  facet_wrap(~reg, nrow = 1)

# Experience (exp)- interaction with Race
plot1 <- ggplot(data, aes(x = exp, y = log(wage), group = race)) +
  geom_smooth(aes(linetype = race, color = race))

# Commute distance (com)
plot2 <- ggplot(data, aes(x = com, y = log(wage))) +
  geom_point() +
  geom_smooth(aes(linetype = race, color = race))

# Education years (edu)
plot3<- ggplot(data, aes(x = edu, y = log(wage))) +
  geom_point() +
  geom_smooth(aes(linetype = race, color = race)) 

# Number of Employees (emp)
plot4 <- ggplot(data, aes(x = emp, y = log(wage))) +
  geom_point()+
  geom_smooth(aes(group = race, linetype = race, color = race))

grid.arrange(plot1, plot2, plot3, plot4, ncol=2)

# City (city) by Race (race)
plot5 <- ggplot(data, aes(x = city, y = log(wage))) +
  geom_boxplot() +
  facet_wrap(~race, nrow = 1)

# Degree (deg) by Race (race)
plot6<- ggplot(data, aes(x = deg, y = log(wage))) +
  geom_boxplot() +
  facet_wrap(~race, nrow = 1)

grid.arrange(plot5, plot6, nrow=1)


par(mfrow=c(2,4))

exp <- data$exp
edu <- data$edu
city <- data$city
race <- data$race
reg <- data$reg
emp <- data$emp
wage <- data$wage

# City vs. region (yes)
interaction.plot(city,reg,log(wage))


# race vs. region- lines cross
interaction.plot(race,reg,log(wage))


# race vs. city- slightly different slopes (included)
interaction.plot(race,city,log(wage))

#exp:edu

#interaction.plot(exp, edu, log(wage))

#city:edu 
interaction.plot(city, edu, log(wage))

#exp:reg 
interaction.plot(reg, exp, log(wage))

#reg:edu 
interaction.plot(reg, edu, log(wage))

#exp:city 
interaction.plot(city, exp, log(wage))

# race:emp
interaction.plot(race, emp, log(wage))

```

## II. Statistical Model
```{r}
# basic model- all predictors used
#summary(lm(log(wage)~edu+exp+city+reg+race+deg+emp+com,data=train.data))

# 34% 
#summary(lm(log(wage)~edu+I(edu^2)+exp+I(exp^2)+city+reg+race+deg,data=train.data))

#35.38
model <- lm(formula = log(wage) ~ I(edu^2) + exp + I(exp^2) + city + race + 
    reg + emp + edu + exp:edu + city:edu + exp:reg + city:reg + 
    reg:edu + exp:city + race:emp + city:race, data = data)
summary(model)

p <- 16
n <- nrow(data)

#dffits(model)

# DFBETAS would be more appropriate since we are focusing on answering the research questions (testing slopes), not predicting

x <- seq(1, nrow(data))
y.intc <- 2/sqrt(n)
         
dfplot1 <- ggplot(data, aes(x =x, y = dfbetas(model)[,2])) +
  geom_point() +
  geom_hline(aes(yintercept = y.intc, col= "red")) +
  geom_hline(aes(yintercept = -y.intc, col = "red"))+ theme(legend.position="none")
             
dfplot2 <-ggplot(data, aes(x =x, y = dfbetas(model)[,3])) +
  geom_point() +
  geom_hline(aes(yintercept = y.intc, col= "red")) +
  geom_hline(aes(yintercept = -y.intc, col = "red"))+ theme(legend.position="none")
             
dfplot3 <-ggplot(data, aes(x =x, y = dfbetas(model)[,4])) +
  geom_point() +
  geom_hline(aes(yintercept = y.intc, col= "red")) +
  geom_hline(aes(yintercept = -y.intc, col = "red"))+ theme(legend.position="none")
      
dfplot4<-ggplot(data, aes(x =x, y = dfbetas(model)[,5])) +
  geom_point() +
  geom_hline(aes(yintercept = y.intc, col= "red")) +
  geom_hline(aes(yintercept = -y.intc, col = "red"))+ theme(legend.position="none")
      
dfplot5<-ggplot(data, aes(x =x, y = dfbetas(model)[,6])) +
  geom_point() +
  geom_hline(aes(yintercept = y.intc, col= "red")) +
  geom_hline(aes(yintercept = -y.intc, col = "red"))+ theme(legend.position="none")
      
dfplot6<-ggplot(data, aes(x =x, y = dfbetas(model)[,7])) +
  geom_point() +
  geom_hline(aes(yintercept = y.intc, col= "red")) +
  geom_hline(aes(yintercept = -y.intc, col = "red"))+ theme(legend.position="none")

grid.arrange(dfplot1,dfplot2, dfplot3,dfplot4,dfplot5,dfplot6, nrow=2)
             
```

## III. Research Question
### a. Setup
### b. Discussion & Conclusion

## IV. Appendix

### a. Model Selection
#### Transformation
The QQ-plot below that shows the untransformed response looks very bad (barely aligns with the 45 degree line). The model's QQ-plot shows that a log transformation of the response is a good choice. 

```{r}
par(mfrow=c(1,3))
qqnorm(rstudent(lm(wage~edu+exp+city+reg+race+deg+com+emp, data=data)))
qqnorm(rstudent(model))
```


#### Functional Forms
According to EDA, experience and education seem to have quadratic forms. Thus, both predictor variables were squared.

#### Selected and Unselected Variables
The basic model where all predictors were used didn't do very well.
The commute distance did not seem to have an effect the response, according to EDA. 
The college degree information is included in the edu data.

### b. Diagnostics and Model Validation

```{r}
city <- I(data$city=="yes")
white <- I(data$race=="white")
other <- I(data$race=="other")
northeast <- I(data$reg=="northeast")
south <- I(data$reg=="south")
west <- I(data$reg=="west")

lwage <- I(log(data$wage))
lineaeexp <- poly(data$exp,2)[,1] 
exp <- I(data$exp) # same as above
quadexp <- poly(data$exp,2)[,2]
lineaeedu <- poly(data$edu,2)[,1] 
edu <- I(data$edu) #same as above
quadedu <- poly(data$edu,2)[,2]

emp <- I(data$emp)
exp.edu <- I(exp*edu) 
cityyes.edu <- I(city*edu) 
exp.south <- I(exp*south)
exp.northeast <- I(exp*northeast)
exp.west <- I(exp*west)

cityyes.south <- I(city*south)
cityyes.west <- I(city*west)
cityyes.northeast <- I(city*northeast)

south.edu <- I(south*edu)
northeast.edu <- I(northeast*edu)
west.edu <- I(west*edu)

exp.cityyes <- I(exp*city)
white.emp <- I(white*emp)
other.emp <- I(other*emp)
white.cityyes <- I(city*white)
other.cityyes <- I(city*other)


model.manual <- lm(lwage~lineaeexp +quadexp +lineaeedu +quadedu  +city +other +white +northeast +south +west +emp +exp.edu +cityyes.edu +exp.south +exp.northeast +exp.west +cityyes.south +cityyes.west +cityyes.northeast +south.edu +northeast.edu +west.edu +exp.cityyes +white.emp +other.emp +white.cityyes +other.cityyes)
#same as the original final model- just manually inputted predictor
summary(model.manual)

# Define test data frame for test data
city.test <- I(test.data$city=="yes")
white.test <- I(test.data$race=="white")
other.test <- I(test.data$race=="other")
northeast.test <- I(test.data$reg=="northeast")
south.test <- I(test.data$reg=="south")
west.test <- I(test.data$reg=="west")

lwage.test <- I(log(test.data$wage))
lineaeexp.test <- poly(test.data$exp,2)[,1] 
quadexp.test <- poly(test.data$exp,2)[,2]
lineaeedu.test <- poly(test.data$edu,2)[,1] 
quadedu.test <- poly(test.data$edu,2)[,2]
emp.test <- I(test.data$emp)

exp.edu.test <- I(lineaeexp.test*lineaeedu.test) 
cityyes.edu.test <- I(city.test*lineaeedu.test) 
exp.south.test <- I(lineaeexp.test*south.test)
exp.northeast.test <- I(lineaeexp.test*northeast.test)
exp.west.test <- I(lineaeexp.test*west.test)

cityyes.south.test <- I(city.test*south.test)
cityyes.west.test <- I(city.test*west.test)
cityyes.northeast.test <- I(city.test*northeast.test)

south.edu.test <- I(south.test*lineaeedu.test)
northeast.edu.test <- I(northeast.test*lineaeedu.test)
west.edu.test <- I(west.test*lineaeedu.test)

exp.cityyes.test <- I(lineaeexp.test*city.test)
white.emp.test <- I(white.test*emp.test)
other.emp.test <- I(other.test*emp.test)
white.cityyes.test <- I(city.test*white.test)
other.cityyes.test <- I(city.test*other.test)


X.test <- data.frame(city = city.test, 
white = white.test, 
other= other.test, 
northeast= northeast.test, 
south = south.test, 
west= west.test, 
lineaeexp= lineaeexp.test, 
quadexp= quadexp.test, 
lineaeedu= lineaeedu.test, 
quadedu= quadedu.test, 
emp= emp.test, 
exp.edu = exp.edu.test, 
cityyes.edu= cityyes.edu.test, 
exp.south= exp.south.test, 
exp.northeast= exp.northeast.test, 
exp.west= exp.west.test, 
cityyest.south= cityyes.south.test, 
cityyest.west= cityyes.west.test, 
cityyest.northeast= cityyes.northeast.test, 
south.edu= south.edu.test, 
northeast.edu = northeast.edu.test, 
west.edu = west.edu.test, 
exp.cityyest= exp.cityyes.test, 
white.emp= white.emp.test, 
other.emp= other.emp.test, 
white.cityyest= white.cityyes.test, 
other.cityyest= other.cityyes.test)

head(X.test)

y.hat.mspr <- predict(model.manual,newdata = X.test)

# Note that y.hat.mspr has the same length as the test data   
length(y.hat.mspr)

# Compute MSPR
n.star <- nrow(test.data)

MSPR <- 1/n.star*sum((lwage.test-y.hat.mspr)^2)

MSE <- (summary(lm(model.manual))$sigma)^2

# Compare MSPR  to MSE
MSPR
MSE

MSPR/MSE


#bestglm(model.df, Ic="AIC")
#bestglm(model.df, Ic="BIC")

# > bestglm(model.df, Ic="AIC")
# Morgan-Tatar search since factors present with more than 2 levels.
# BIC
# Best Model:
#                Df Sum Sq Mean Sq F value   Pr(>F)    
# city            1    154   153.9  537.12  < 2e-16 ***
# race            2    157    78.5  274.08  < 2e-16 ***
# reg             3     53    17.6   61.44  < 2e-16 ***
# emp             1     26    26.2   91.30  < 2e-16 ***
# edu             1    955   955.3 3333.85  < 2e-16 ***
# exp.edu         1    912   911.7 3181.70  < 2e-16 ***
# city.edu        1     12    11.8   41.11 1.47e-10 ***
# city.reg        1      5     5.0   17.38 3.07e-05 ***
# exp.city        1     49    49.4  172.42  < 2e-16 ***
# Residuals   19845   5687     0.3                     
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# There were 50 or more warnings (use warnings() to see the first 50)
# > bestglm(model.df, Ic="BIC")
# Morgan-Tatar search since factors present with more than 2 levels.
# BIC
# Best Model:
#                Df Sum Sq Mean Sq F value   Pr(>F)    
# city            1    154   153.9  537.12  < 2e-16 ***
# race            2    157    78.5  274.08  < 2e-16 ***
# reg             3     53    17.6   61.44  < 2e-16 ***
# emp             1     26    26.2   91.30  < 2e-16 ***
# edu             1    955   955.3 3333.85  < 2e-16 ***
# exp.edu         1    912   911.7 3181.70  < 2e-16 ***
# city.edu        1     12    11.8   41.11 1.47e-10 ***
# city.reg        1      5     5.0   17.38 3.07e-05 ***
# exp.city        1     49    49.4  172.42  < 2e-16 ***
# Residuals   19845   5687     0.3                     
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# There were 50 or more warnings (use warnings() to see the first 50)

```

### c. Influential Observations and Collinearity
```{r}
cor(data)

vif(model)
```

## d. Discussion
```{r}
summary(lm(formula = log(wage) ~ I(edu^2) + exp + I(exp^2) + city + race +
  emp + reg + deg + edu + city:reg + exp:city + exp:reg + exp:emp +
  city:deg + exp:deg + race:emp + race:deg + reg:deg + city:edu +
  exp:edu + reg:edu + exp:race + race:edu + emp:edu + exp:city:edu +
  city:reg:edu + race:emp:edu + exp:race:edu, data = data))$adj.r.squared
```
This model had the highest adjusted R-squared, but I didn't employ this as the final model because it is far from parsimonious or interpretable. Besides, it is only an 0.02% increase in adjusted R-squred from my final model.

A way to increase adjusted R-squared would be to brute force it, where you try every possible combination of interactions and higher order terms and keep your computer running forever until you reach a target value (say, 50%). It would be interesting to see what model you would get, but I assume it won't be very interpretable.
