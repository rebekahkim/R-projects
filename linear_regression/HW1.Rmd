---
title: 'STAT UN2103 Section 001 Assignment #1'
author: "Rebekah Kim"
date: "September 20, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1

i. The linear regression assumes 
  $Y_i = \beta_0 + \beta_1X_1 + \epsilon_i$, i = 1, 2, ... n
    
    where $\epsilon_i$ is independent and identifically distributed N(0, $\sigma^2$).
    
  Glancing at the fungus growth vs. laetisaric acid graph, there seems to be a linear relationship between the two variables; Indeed the variance of the error residuals ($e_i$) seems constant (the data points have similar lengths of deviation from the regression line). Also, the Q-Q plot further shows that it is plausible to assume that $\epsilon_i$ is normal with constant variance ($\epsilon_i \stackrel{iid}\sim N(0, \sigma^2)$).


ii.$\hat{\beta}_1 = \dfrac{S_{xy}}{S_{xx}}$

$S_{xy} = \sum\limits_{i=1}^n (x_i-\bar{x})(y_i-\bar{y}) = -927.75$

$S_{xx} = \sum\limits_{i=1}^n (x_i-\bar{x})^2 = 1303$

$\hat{\beta}_1 = \dfrac{-927.75}{1303} = -0.712010744$

There is a relatively strong, negative (-0.71) linear relationship.


iii. coefficient of determination = $r^2 = 1 - \dfrac{SSE}{SST}$

$SSE = \sum\limits_{i=1}^n (y-\hat{y})^2 = 16.78$

$SST = S_{yy} = \sum\limits_{i=1}^n (y-\bar{y})^2 = 677.35$

$r^2 = 1 - \dfrac{SSE}{SST} = 1- 16.78/677.35 = 0.975226988$

$r^2$ is close to 0.98, meaning 98% of variation in fungus growth is explained by the laetisaric acid concentration. The acid compound probably effectively controls the fungus growth.


iv. line of best fit ($\hat{y}$) = $\hat{\beta}_0 + \hat{\beta}_1x$

$\hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x}$ = 23.64 - (-0.712010744)*11.50 = 31.8281236

$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1x$ =  31.8281236 -0.712010744x

MSE = $\dfrac{SSE}{n-2} = \dfrac{16.78}{12-2}$ = 1.678



v. For every 1 $\mu$ G/mL increase in laetisaric acid concentration, the average fungus growth (radial measurement of the colony) decrease by 0.7120107mm



vi. 31.8281236 -0.712010744*15 = 21.14796 mm of fungal growth

## Problem 2
i. Use R to find the line of best fit for this application.
```{r}
#dataset
alcohol <- c(6.47, 6.13, 6.19, 4.89, 5.63, 4.52, 5.89, 4.79, 5.27, 6.08, 4.02)
tobacco <- c(4.03, 3.76, 3.77, 3.34, 3.47, 2.92, 3.20, 2.71, 3.53, 4.51, 4.56)

#linear model
model = lm(alcohol~tobacco)

#find beta 1 (slope) and beta 0 (intercept) of line of best fit
model
```
line of best fit ($\hat{y}$) = $\hat{\beta}_0 + \hat{\beta}_1x$
  = 4.3512 + 0.3019x


ii. Use R to find the correlation coefficient for this application.
```{r}
cor(tobacco, alcohol)
```

iii. Use R to create a scatter plot with the line of best fit. Make the line of best fit red.
```{r}
plot(tobacco, alcohol, main="Alcohol vs Tobacco expenditure")
abline(model, col=2)
```

iv. After looking at the scatter plot, do you think this line will do a good job predicting
alcohol consumption for a fixed level of tobacco usage?
```{r}
summary(model)

```

No this line will probably not do a good job because there is weak correlation (0.2236), low R-squared value (0.05), and a high p-value (0.5087).

## Problem 3
Consider a data set (x1, y1),(x2, y2),...,(xn, yn) and suppose yi = 2xi + 1 for i = 1, 2,...,n.
Show that the sample correlation coefficient r is equal to 1.

```{r}
x <- 1:1000000
y <- 2 * x + 1
cor(x, y)
```

It is the property of the sample correlation that r = 1 or -1 iff y = ax + b for some real number a, b with a != 0. Here, a = 2, b = 1.

## Problem 4

i.
```{r}

#datasets
control <- c(808,757,773,937,726,788,806,792,751,765,853,655,626,721,630,722,683,709,718,812,703,791,586,864,737,701,799,844,639,705,822,935,842,827,784,838,795,823,791,819) 
cell <- c(909,712,805,852,859,781,841,822,740,910,900,912,863,785,863,809,927,847,918,810,788,929,798,863,981,842,1021,827,876,736,640,851,787,703,942,758,843,781,969,872)

#Exploratory Analysis
#Descriptive Statistics on control and cellphone datasets
summary(control)
var(control)

summary(cell)
var(cell)

#Histogram
hist(control)
hist(cell)

#Stacked data
y <- c(control,cell)

#Sample sizes
m <- length(control)
n <- length(cell)

#Box plot

boxplot(y~c(rep("control", m), rep("cell phone", n)), main="Box plot", ylab = "Reaction time")

```

The boxplot seems to support the research question that cell phone users have a longer reaction time than the control group, as the median and the mean of the control group reaction time is below that of the first quartile of the cell phone group.

ii. 

```{r}

#Assessing normality 
h <- hist(y,main="Histogram (All Reaction time data)")
x <- 50:150/10 

# set graphics window to plot side by side
par(mfrow=c(2,2))

qqnorm((cell-mean(cell))/sd(cell),main="Normal QQ Plot (Cell phone)")
abline(a=0,b=1,lty=2)
qqnorm((control-mean(control))/sd(control),main="Normal QQ Plot (Control)")
abline(a=0,b=1,lty=2)
qqnorm((y-mean(y))/sd(y),main="Normal QQ Plot (All Reaction time data)")
abline(a=0,b=1,lty=2)
plot(h,freq=F,main="Histogram (All Reaction time data)")
curve(dnorm(x, mean=mean(y), sd=sd(y)), add=TRUE,col=2)
```

In order to perform the relevant hypothesis test, we need to make sure that the distribution of the data sets show normality. Upon observing the graphs, it is plausible to assume that normality of the control, cell phone, and combined data distributions, since the data points closely align with the 45 degree line in the QQ-plots and the histogram closely exhibits normal distribution. 


iii. 

```{r}
# The null hypothesis is mean of the control = mean of cell or mean_control - mean_cell = 0
# The alternative hpothesis is mean of the control < mean of cell or mean_control - mean_cell < 0

#two sample t-test for means
t.test(control, cell, alternative = "less")
```

p-value =  3.358e-05 < 0.05. Therefore we can reject $H_0$


iv. 

```{r}

#check variance do not statistically differ per group
# H_0: variance of control = variance of cell, H_A: variance of control != variance of cell
library(car)
leveneTest(y~factor(c(rep("control", m), rep("cell phone", n))))

#p-value 0.9318 > 0.1 therefore, we fail to reject H_0
#It is reasonable to assume equal variances per group

# pooled two sample T-test
t.test( control,cell, alternative = "less", var.equal=TRUE)
```

p-value = 3.358e-05 < 0.05. Therefore we can reject $H_0$


v. We may have committed Type I error because we may have rejected the null hypothesis (mean of the control = mean of cell) when actually the null hypothesis is true.


vi. We should use the pooled t-test. It meets the requirement: Levene's test suggested that it is reasonable to assume equal variances per group. Using the pooled t-test, we can have higher precision and can translate it to regression. 